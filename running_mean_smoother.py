# -*- coding: utf-8 -*-
"""Running Mean Smoother.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OCB9HgIbzmuEgWJc51bikR2ueOYKjs0X
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import os
import kagglehub

# Download dataset using kagglehub
path = kagglehub.dataset_download("whoishmk/counter-strike-top-50-players-stats")
csv_file = [f for f in os.listdir(path) if f.endswith(".csv")][0]
df = pd.read_csv(os.path.join(path, csv_file))

# Features and target
features = ["Maps", "Rounds", "K-D\xa0Diff", "K/D"]
target = "Rating1.0"
X = df[features].values
Y = df[target].values.reshape(-1, 1)

# Show first 5 samples
print("First 5 Samples from Dataset:")
print(df.head(5))

# Show random 5 samples
print("\nRandom 5 Samples from Dataset:")
print(df.sample(5, random_state=42))


# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Splitting
X_temp, X_test, Y_temp, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)
X_train, X_val, Y_train, Y_val = train_test_split(X_temp, Y_temp, test_size=0.25, random_state=42)

X_trainval = np.vstack((X_train, X_val))
Y_trainval = np.vstack((Y_train, Y_val))

# Box kernel weight function
def weight_function(u):
    return np.where(np.abs(u) < 1, 1, 0)

# Running Mean Smoother
def running_mean_smoother(X_trainval, Y_trainval, X_eval, h):
    smoothed_Y = []
    fallback_value = np.mean(Y_trainval)
    for x in X_eval:
        distances = np.linalg.norm((x - X_trainval) / h, axis=1)
        weights = weight_function(distances)
        numerator = np.sum(weights * Y_trainval.flatten())
        denominator = np.sum(weights)
        smoothed_val = fallback_value if denominator == 0 else numerator / denominator
        smoothed_Y.append(smoothed_val)
    return np.array(smoothed_Y).reshape(-1, 1)

# Metrics
def compute_metrics(Y_true, Y_pred, n_features):
    mse = mean_squared_error(Y_true, Y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(Y_true, Y_pred)
    r2 = r2_score(Y_true, Y_pred) * 100
    mape = np.mean(np.abs((Y_true - Y_pred) / Y_true)) * 100
    adj_r2 = (1 - (1 - r2 / 100) * (len(Y_true) - 1) / (len(Y_true) - n_features - 1)) * 100
    return mse, rmse, mae, r2, adj_r2, mape

# Bin sizes
bin_sizes = [0.1, 0.5, 0.7758, 1, 2, 5]
metrics_results = []
predictions = {}

# Run smoother for each bin size
for h in bin_sizes:
    Y_pred_test = running_mean_smoother(X_trainval, Y_trainval, X_test, h)
    mse, rmse, mae, r2, adj_r2, mape = compute_metrics(Y_test, Y_pred_test, X.shape[1])
    metrics_results.append([h, r2, adj_r2, mape, mae, rmse,mse])
    predictions[h] = Y_pred_test

# Results DataFrame
results_df = pd.DataFrame(metrics_results, columns=["BinSize", "Accuracy (R2%)", "Adj Accuracy (AdjR2%)", "MAPE (%)", "MAE", "RMSE","MSE"])
print("\nMetrics for Different Bin Sizes:")
print(results_df)

# Graph 1: Window Width vs Error
plt.figure(figsize=(10, 6))
plt.plot(results_df["BinSize"], results_df["MAPE (%)"], label="MAPE (%)", marker='o')
plt.plot(results_df["BinSize"], results_df["MAE"], label="MAE", marker='s')
plt.plot(results_df["BinSize"], results_df["RMSE"], label="RMSE", marker='^')
plt.xlabel("Window Width (Bin Size)")
plt.ylabel("Error")
plt.title("Graph 1: Window Width vs Error Metrics")
plt.grid(True)
plt.legend()
plt.show()

# Graph 2: Accuracy
plt.figure(figsize=(10, 6))
plt.plot(results_df["BinSize"], results_df["Accuracy (R2%)"], label="Accuracy (R²%)", marker='o')
plt.plot(results_df["BinSize"], results_df["Adj Accuracy (AdjR2%)"], label="Adjusted Accuracy (AdjR²%)", marker='s')
plt.xlabel("Window Width (Bin Size)")
plt.ylabel("Accuracy (%)")
plt.title("Graph 2: Accuracy vs Window Width")
plt.grid(True)
plt.legend()
plt.show()

# Actual vs Predicted Scatter Plot
plt.figure(figsize=(10, 6))
colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']
for i, h in enumerate(bin_sizes):
    plt.scatter(Y_test, predictions[h], alpha=0.6, color=colors[i], label=f"h = {h}", edgecolor='k')

plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'black', linestyle='--', linewidth=2)
plt.xlabel("Actual Rating1.0")
plt.ylabel("Predicted Rating1.0")
plt.title("Actual vs Predicted (Test Set) for Various Bin Sizes")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Final Metrics Display
for i, h in enumerate(bin_sizes):
    print(f"\nBin Size: {h}")
    print(f"Test Accuracy (R²): {results_df.iloc[i]['Accuracy (R2%)']:.2f}%")
    print(f"Test MAE: {results_df.iloc[i]['MAE']:.5f}")

# Best and Worst predictions for h = 0.7758
h_target = 0.7758
Y_pred_selected = predictions[h_target]
errors = np.abs(Y_test - Y_pred_selected)


# Combine for sorting
pred_df = pd.DataFrame({
    "Actual": Y_test.flatten(),
    "Predicted": Y_pred_selected.flatten(),
    "Absolute Error": errors.flatten()
})

sorted_df = pred_df.sort_values(by="Absolute Error")

print("\nTop 5 Best Predictions (Lowest Error) for h = 0.7758:")
print(sorted_df.head(5).to_string(index=False))

print("\nTop 5 Worst Predictions (Highest Error) for h = 0.7758:")
print(sorted_df.tail(5).to_string(index=False))